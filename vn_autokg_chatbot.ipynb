{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cypher\n",
    "MATCH (n) DETACH DELETE n;\n",
    "```\n",
    "\n",
    "```bash\n",
    "docker-compose up -d --build\n",
    "```\n",
    "\n",
    "```bash\n",
    "docker-compose down\n",
    "```\n",
    "\n",
    "```cypher\n",
    "MATCH (n)\n",
    "RETURN n\n",
    "```\n",
    "\n",
    "```cypher\n",
    "SHOW INDEXES;\n",
    "DROP INDEX vector;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jPIRSGz4tHNV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_19304\\1502321853.py:19: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from vn_langchainLGT import LLMGraphTransformer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceed\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langchain_core.runnables import  RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.graphs.neo4j_graph import Neo4jGraph\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    UnstructuredFileLoader,\n",
    "    JSONLoader,\n",
    ")\n",
    "\n",
    "from vn_langchainLGT import LLMGraphTransformer\n",
    "from typing import Sequence\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import os\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from neo4j import  Driver\n",
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "import pdfplumber\n",
    "from docx import Document as DocxDocument\n",
    "import docx\n",
    "from markdown import markdown\n",
    "\n",
    "import pickle\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if not load_dotenv():\n",
    "    print(\"Warning: .env not correctly set up\")\n",
    "else:\n",
    "    print(\"Proceed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"aya-23-8b@q8_0\"\n",
    "EMB_MODEL = \"text-embedding-multilingual-e5-large-instruct\"\n",
    "CHECKPOINT_PATH = \"vi_processing_checkpoint.json\"\n",
    "DOC_DIR = \"tailieu/\"\n",
    "ALLOWED_NODES = []\n",
    "ALLOWED_RELATIONSHIPS = []\n",
    "STRICT_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB is not populated\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "def is_database_populated(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"your_password\"):\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"MATCH (n) RETURN count(n) AS node_count\")\n",
    "        node_count = result.single()[\"node_count\"]\n",
    "        \n",
    "    driver.close()\n",
    "    return node_count > 0\n",
    "\n",
    "if not is_database_populated():\n",
    "    print(\"DB is not populated\")\n",
    "else:\n",
    "    print(\"DB IS POPULATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 23:15:50,368 - INFO - Processed tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md into 11 chunks00:00<?, ?it/s]\n",
      "Processing files: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 123.39it/s]\n",
      "2025-03-19 23:15:58,320 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\", ?it/s]\n",
      "2025-03-19 23:15:58,320 - INFO - Converted document tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md_-7808350393287397044 to GraphDocument.\n",
      "2025-03-19 23:16:09,150 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\".95s/it]\n",
      "2025-03-19 23:16:09,150 - INFO - Converted document tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md_-2372475016998018864 to GraphDocument.\n",
      "2025-03-19 23:16:22,929 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\".64s/it]\n",
      "2025-03-19 23:16:22,943 - INFO - Converted document tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md_-1153490466340006883 to GraphDocument.\n",
      "2025-03-19 23:16:35,970 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\".54s/it]\n",
      "2025-03-19 23:16:35,970 - INFO - Converted document tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md_291537954653993696 to GraphDocument.\n",
      "2025-03-19 23:16:50,848 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\".13s/it]\n",
      "2025-03-19 23:16:50,848 - INFO - Converted document tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md_1431784117483759977 to GraphDocument.\n",
      "2025-03-19 23:17:04,871 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\".12s/it]\n",
      "2025-03-19 23:17:04,884 - INFO - Converted document tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md_-2586087896414734458 to GraphDocument.\n",
      "2025-03-19 23:17:21,592 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\".43s/it]\n",
      "2025-03-19 23:17:21,601 - INFO - Converted document tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md_-2143385549389118738 to GraphDocument.\n",
      "2025-03-19 23:17:33,685 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\".51s/it]\n",
      "2025-03-19 23:17:33,695 - INFO - Converted document tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md_7406471480328360734 to GraphDocument.\n",
      "2025-03-19 23:17:42,537 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\".74s/it]\n",
      "2025-03-19 23:17:42,537 - INFO - Converted document tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md_-5001988362331979198 to GraphDocument.\n",
      "2025-03-19 23:17:54,561 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\".21s/it]\n",
      "2025-03-19 23:17:54,561 - INFO - Converted document tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md_5771214686641010325 to GraphDocument.\n",
      "2025-03-19 23:18:03,991 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\".15s/it]\n",
      "2025-03-19 23:18:03,991 - INFO - Converted document tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md_2081262470910820209 to GraphDocument.\n",
      "Converting to graph: 100%|█████████████████████████████████████████████████████████████| 11/11 [02:13<00:00, 12.15s/it]\n",
      "2025-03-19 23:18:04,682 - INFO - Inserted document -7808350393287397044                         | 0/11 [00:00<?, ?it/s]\n",
      "2025-03-19 23:18:05,030 - INFO - Inserted document -2372475016998018864                 | 1/11 [00:00<00:06,  1.47it/s]\n",
      "2025-03-19 23:18:05,301 - INFO - Inserted document -1153490466340006883                 | 2/11 [00:01<00:04,  2.06it/s]\n",
      "2025-03-19 23:18:05,915 - INFO - Inserted document 291537954653993696                   | 3/11 [00:01<00:03,  2.58it/s]\n",
      "2025-03-19 23:18:06,582 - INFO - Inserted document 1431784117483759977                  | 4/11 [00:01<00:03,  2.10it/s]\n",
      "2025-03-19 23:18:07,145 - INFO - Inserted document -2586087896414734458                 | 5/11 [00:02<00:03,  1.83it/s]\n",
      "2025-03-19 23:18:07,808 - INFO - Inserted document -2143385549389118738                 | 6/11 [00:03<00:02,  1.82it/s]\n",
      "2025-03-19 23:18:08,437 - INFO - Inserted document 7406471480328360734                  | 7/11 [00:03<00:02,  1.70it/s]\n",
      "2025-03-19 23:18:08,821 - INFO - Inserted document -5001988362331979198                 | 8/11 [00:04<00:01,  1.66it/s]\n",
      "2025-03-19 23:18:09,321 - INFO - Inserted document 5771214686641010325██████▋           | 9/11 [00:04<00:01,  1.88it/s]\n",
      "2025-03-19 23:18:09,896 - INFO - Inserted document 2081262470910820209███████████▍     | 10/11 [00:05<00:00,  1.91it/s]\n",
      "Inserting documents: 100%|█████████████████████████████████████████████████████████████| 11/11 [00:05<00:00,  1.87it/s]\n",
      "2025-03-19 23:18:09,896 - INFO - Processing completed successfully\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "class CheckpointManager:\n",
    "    def __init__(self, path: str = CHECKPOINT_PATH):\n",
    "        self.path = path\n",
    "        self.data: Dict[str, Any] = {\n",
    "            \"processed_files\": [],\n",
    "            \"converted_chunks\": [],\n",
    "            \"inserted_documents\": [],  # Track inserted documents by doc_id\n",
    "            \"inserted_batches\": [],  # Track batch indices for logging purposes\n",
    "            \"processed_graph_docs\": [],  # New field to track processed graph docs by doc_id\n",
    "        }\n",
    "        if os.path.exists(path):\n",
    "            self.load()\n",
    "\n",
    "    def load(self):\n",
    "        with open(self.path, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    def save(self):\n",
    "        with open(self.path, \"w\") as f:\n",
    "            json.dump(self.data, f)\n",
    "\n",
    "    def is_file_processed(self, file_path: str, file_hash: int) -> bool:\n",
    "        \"\"\"Check if a file has been processed by its hash.\"\"\"\n",
    "        return file_hash in self.data[\"processed_files\"]\n",
    "\n",
    "    def add_processed_file(self, file_path: str, file_hash: int):\n",
    "        \"\"\"Mark a file as processed by its hash.\"\"\"\n",
    "        if file_hash not in self.data[\"processed_files\"]:\n",
    "            self.data[\"processed_files\"].append(file_hash)\n",
    "            self.save()\n",
    "\n",
    "    def add_converted_chunk(self, chunk_hash: int):\n",
    "        \"\"\"Mark a chunk as converted by its hash.\"\"\"\n",
    "        if chunk_hash not in self.data[\"converted_chunks\"]:\n",
    "            self.data[\"converted_chunks\"].append(chunk_hash)\n",
    "            self.save()\n",
    "\n",
    "    def is_graph_doc_processed(self, doc_id: str) -> bool:\n",
    "        \"\"\"Check if a graph document has been processed by its doc_id.\"\"\"\n",
    "        return doc_id in self.data[\"processed_graph_docs\"]\n",
    "\n",
    "    def add_processed_graph_doc(self, doc_id: str):\n",
    "        \"\"\"Mark a graph document as processed by its doc_id.\"\"\"\n",
    "        if doc_id not in self.data[\"processed_graph_docs\"]:\n",
    "            self.data[\"processed_graph_docs\"].append(doc_id)\n",
    "            self.save()\n",
    "\n",
    "    def is_document_inserted(self, doc_hash: int) -> bool:\n",
    "        \"\"\"Check if a document has been inserted into Neo4j by its doc_hash.\"\"\"\n",
    "        return doc_hash in self.data[\"inserted_documents\"]\n",
    "\n",
    "    def add_inserted_document(self, doc_id: str):\n",
    "        \"\"\"Mark a document as inserted into Neo4j by its doc_id.\"\"\"\n",
    "        if doc_id not in self.data[\"inserted_documents\"]:\n",
    "            self.data[\"inserted_documents\"].append(doc_id)\n",
    "            self.save()\n",
    "\n",
    "    def add_inserted_batch(self, batch_id: int):\n",
    "        \"\"\"Mark a batch as inserted.\"\"\"\n",
    "        if batch_id not in self.data[\"inserted_batches\"]:\n",
    "            self.data[\"inserted_batches\"].append(batch_id)\n",
    "            self.save()\n",
    "\n",
    "\n",
    "def strip_markdown(text: str) -> str:\n",
    "    \"\"\"Simplify markdown text to plain text\"\"\"\n",
    "    html = markdown(text)\n",
    "    return re.sub(r'<.*?>', '', html)\n",
    "\n",
    "def extract_json_strings(data: Any) -> str:\n",
    "    \"\"\"Recursively extract all strings from JSON data\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return \" \".join(extract_json_strings(v) for v in data.values())\n",
    "    if isinstance(data, list):\n",
    "        return \" \".join(extract_json_strings(v) for v in data)\n",
    "    return str(data) if isinstance(data, str) else \"\"\n",
    "\n",
    "def extract_pdf_text(file_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF using PyPDF2\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        pdf_text = []\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                pdf_text.append(text)\n",
    "        return \"\\n\".join(pdf_text)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting PDF text from {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_docx_text(file_path: str) -> str:\n",
    "    \"\"\"Extract text from DOCX file using python-docx\"\"\"\n",
    "    try:\n",
    "        doc = DocxDocument(file_path)\n",
    "        doc_text = []\n",
    "        for para in doc.paragraphs:\n",
    "            doc_text.append(para.text)\n",
    "        return \"\\n\".join(doc_text)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting DOCX text from {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def load_and_clean(file_path: str) -> str:\n",
    "    \"\"\"Load and clean file content based on file type\"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    try:\n",
    "        if ext == \".md\":\n",
    "            # Markdown files - convert to plain text\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                markdown_content = f.read()\n",
    "            return strip_markdown(markdown_content)\n",
    "        elif ext == \".json\":\n",
    "            # JSON files - extract strings from the JSON object\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            return extract_json_strings(data)\n",
    "        elif ext == \".pdf\":\n",
    "            # PDF files - extract text using PyPDF2\n",
    "            return extract_pdf_text(file_path)\n",
    "        elif ext == \".docx\":\n",
    "            # DOCX files - extract text using python-docx\n",
    "            return extract_docx_text(file_path)\n",
    "        elif ext == \".txt\":\n",
    "            # For text files, just load directly\n",
    "            loader = TextLoader(file_path)\n",
    "            return loader.load()[0].page_content\n",
    "        else:\n",
    "            logging.warning(f\"Unsupported file type: {file_path}\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "###### PIPELINE\n",
    "checkpoint = CheckpointManager()\n",
    "graph = Neo4jGraph()\n",
    "client = ChatOpenAI(\n",
    "    base_url=\"http://127.0.0.1:8000/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=MODEL,\n",
    "    temperature=0,\n",
    "    request_timeout=240,\n",
    ")\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(\n",
    "    llm=client,\n",
    "    allowed_nodes=ALLOWED_NODES,\n",
    "    allowed_relationships=ALLOWED_RELATIONSHIPS,\n",
    "    strict_mode=STRICT_MODE,\n",
    ")\n",
    "\n",
    "# File discovery and sorting\n",
    "file_paths = sorted(\n",
    "    [\n",
    "        os.path.join(root, f)\n",
    "        for root, _, files in os.walk(DOC_DIR)\n",
    "        if \".ipynb_checkpoints\" not in root\n",
    "        for f in files\n",
    "        if f.lower().endswith((\"md\", \"json\", \"docx\", \"pdf\", \"txt\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Text processing pipeline\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=75,\n",
    "    chunk_overlap=15,\n",
    "    length_function=lambda x: len(x.split()),\n",
    "    separators=[\" \"]  # Only split by spaces, no newlines\n",
    ")\n",
    "\n",
    "documents: List[Document] = []\n",
    "\n",
    "# Stage 1: File loading and splitting\n",
    "for file_path in tqdm(file_paths, desc=\"Processing files\"):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        content = f.read()\n",
    "    file_hash = hash(content)\n",
    "\n",
    "    # Check if the file has been processed already by checking the content hash\n",
    "    if checkpoint.is_file_processed(file_path, file_hash):\n",
    "        logging.info(f\"File {file_path} has already been processed.\")\n",
    "        continue  # Skip if the file has already been processed\n",
    "\n",
    "    try:\n",
    "        # Load and clean file content\n",
    "        content = load_and_clean(file_path)\n",
    "        if not content:\n",
    "            logging.warning(f\"File {file_path} is empty after cleaning. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Split the document into chunks\n",
    "        chunks = text_splitter.create_documents([content])\n",
    "\n",
    "        # Add metadata (filename and any other relevant details)\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata = {\n",
    "                'source': file_path,  # Store the filename in the chunk metadata\n",
    "                'hash': file_hash  # Store the hash of the file content for comparison\n",
    "            }\n",
    "\n",
    "        documents.extend(chunks)\n",
    "\n",
    "        # Add file to checkpoint with content hash\n",
    "        checkpoint.add_processed_file(file_path, file_hash)  # Ensure file_hash is stored\n",
    "        logging.info(f\"Processed {file_path} into {len(chunks)} chunks\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process {file_path}: {e}\")\n",
    "        continue  # Skip this file and continue with others\n",
    "\n",
    "# Stage 2: Graph document conversion\n",
    "graph_documents = []\n",
    "for doc in tqdm(documents, desc=\"Converting to graph\"):\n",
    "    doc_id = f\"{doc.metadata.get('source', 'unknown')}_{hash(doc.page_content)}\"\n",
    "\n",
    "    # Check if this graph document has already been processed\n",
    "    if checkpoint.is_graph_doc_processed(doc_id):\n",
    "        logging.info(f\"Graph document {doc_id} has already been processed.\")\n",
    "        continue\n",
    "\n",
    "    max_retries = 5\n",
    "    retry_delay = 1  # Seconds before retrying\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            # Attempt to convert the document\n",
    "            graph_doc = llm_transformer.convert_to_graph_documents([doc])[0]\n",
    "\n",
    "            # Assign original metadata to GraphDocument\n",
    "            graph_doc.source = doc\n",
    "            graph_documents.append(graph_doc)\n",
    "\n",
    "            # Mark this graph document as processed\n",
    "            checkpoint.add_processed_graph_doc(doc_id)\n",
    "            logging.info(f\"Converted document {doc_id} to GraphDocument.\")\n",
    "            break  # Success, exit retry loop\n",
    "\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            logging.warning(f\"Attempt {attempt}/{max_retries} failed for {doc_id}: {e}\")\n",
    "\n",
    "            if attempt < max_retries:\n",
    "                sleep(retry_delay)  # Wait before retrying\n",
    "            else:\n",
    "                logging.error(f\"Failed to convert document {doc_id} after {max_retries} attempts.\")\n",
    "                break  # Stop retrying after max attempts\n",
    "\n",
    "# Stage 3: Add to Neo4j\n",
    "batch_size = 10\n",
    "for doc in tqdm(graph_documents, desc=\"Inserting documents\"):\n",
    "    doc_hash = hash(doc.source.page_content)\n",
    "\n",
    "    if checkpoint.is_document_inserted(doc_hash):\n",
    "        logging.info(f\"Document with hash {doc_hash} has already been inserted.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        source = doc.source.metadata.get('source', 'unknown')\n",
    "        file_hash = doc.source.metadata.get('hash', doc_hash)\n",
    "        doc_id = str(doc_hash)\n",
    "\n",
    "        # Insert or update the Document node with page_content\n",
    "        doc_query = f\"\"\"\n",
    "        MERGE (doc:Document {{id: '{doc_id}'}})\n",
    "        ON CREATE SET doc.source = '{source}', doc.hash = {file_hash}, doc.page_content = '{doc.source.page_content}'\n",
    "        ON MATCH SET doc.page_content = '{doc.source.page_content}', doc.updated = timestamp()\n",
    "        \"\"\"\n",
    "        graph.query(doc_query)\n",
    "\n",
    "        for node in doc.nodes:\n",
    "            # Apply only the __Entity__ label to all nodes\n",
    "            node_query = f\"\"\"\n",
    "            MERGE (n:__Entity__ {{id: '{node.id}'}})\n",
    "            ON CREATE SET n.type = '{node.type}'\n",
    "            \"\"\"\n",
    "            graph.query(node_query)\n",
    "            \n",
    "            # Linking Document to Node\n",
    "            link_doc_node_query = f\"\"\"\n",
    "            MATCH (doc:Document {{id: '{doc_id}'}})\n",
    "            MATCH (n:__Entity__ {{id: '{node.id}'}})\n",
    "            MERGE (doc)-[:Nhắc_đến]->(n)\n",
    "            \"\"\"\n",
    "            graph.query(link_doc_node_query)\n",
    "        \n",
    "        for rel in doc.relationships:\n",
    "            rel_query = f\"\"\"\n",
    "            MERGE (source:__Entity__ {{id: '{rel.source.id}'}})\n",
    "            MERGE (target:__Entity__ {{id: '{rel.target.id}'}})\n",
    "            MERGE (source)-[r:{rel.type}]->(target)\n",
    "            \"\"\"\n",
    "            graph.query(rel_query)\n",
    "\n",
    "        checkpoint.add_inserted_document(doc_id)\n",
    "        logging.info(f\"Inserted document {doc_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to insert document {doc_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "logging.info(\"Processing completed successfully\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GHbJPMfDtHNW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = requests.post(\n",
    "        \"http://127.0.0.1:8000/v1/embeddings\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        json={\"input\": text, \"model\": EMB_MODEL}\n",
    "    )\n",
    "    return response.json()[\"data\"][0][\"embedding\"]\n",
    "\n",
    "# Custom embedding function\n",
    "class LMStudioEmbeddings:\n",
    "    def embed_documents(self, texts):\n",
    "        return [get_embedding(text) for text in texts]\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        return get_embedding(text)\n",
    "\n",
    "# Use LM Studio for embeddings\n",
    "embeddings = LMStudioEmbeddings()\n",
    "\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    embeddings,\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"page_content\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")\n",
    "\n",
    "vector_retriever = vector_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817,
     "referenced_widgets": [
      "8e37edd9789a4d57a7be401628e7ff7f",
      "9bac7003afd84cecb4e67a81a396ec8d"
     ]
    },
    "id": "RMZlhtDmtHNW",
    "outputId": "86efa842-3297-45d6-dab2-681bbc836b4d"
   },
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
    "                os.environ[\"NEO4J_PASSWORD\"]))\n",
    "\n",
    "def create_fulltext_index(tx):\n",
    "    query = '''\n",
    "CREATE FULLTEXT INDEX `fulltext_entity_index` \n",
    "FOR (n:__Entity__) \n",
    "ON EACH [n.id, n.type];\n",
    "    '''\n",
    "    tx.run(query)\n",
    "\n",
    "# Function to execute the query\n",
    "def create_index():\n",
    "    with driver.session() as session:\n",
    "        session.execute_write(create_fulltext_index)\n",
    "        print(\"Fulltext index created successfully.\")\n",
    "\n",
    "# Call the function to create the index\n",
    "try:\n",
    "    create_index()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Close the driver connection\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6yCMz_sRtHNW",
    "outputId": "f533f279-9a2b-48d6-830b-28d04c43550b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 23:18:35,657 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names=['thầy Lê Hồng Phương', 'xử lý ngôn ngữ tự nhiên']\n"
     ]
    }
   ],
   "source": [
    "# Define the Entities model expecting 'names' field\n",
    "class Entities(BaseModel):\n",
    "    names: list[str] = Field(..., description=\"Extracted entities from the text.\")\n",
    "\n",
    "# Setup LLM model\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=\"http://127.0.0.1:8000/v1\",\n",
    "    openai_api_key=\"lm-studio\",\n",
    "    model=MODEL,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Bạn hãy trích xuất tất cả các tên người, đối tượng, khái niệm, cơ quan, tổ chức, vật thể, môn học, lĩnh vực có trong câu hỏi dưới đây.\n",
    "Trả về kết quả dưới dạng một đối tượng JSON với trường \"entities\" chứa danh sách các thực thể trích xuất được.\n",
    "LƯU Ý: CHỈ TRẢ VỀ ĐỐI TƯỢNG JSON. KHÔNG GIẢI THÍCH GÌ THÊM.\n",
    "\n",
    "#####\n",
    "VÍ DỤ:\n",
    "\n",
    "Câu hỏi: \"Ai là tổng thống của Hoa Kỳ?\"\n",
    "Đầu ra: {{ \"entities\": [\"tổng thống\", \"Hoa Kỳ\"] }}\n",
    "\n",
    "Câu hỏi: \"Công ty Apple sản xuất những sản phẩm gì?\"\n",
    "Đầu ra: {{ \"entities\": [\"Apple\", \"sản phẩm\"] }}\n",
    "\n",
    "Câu hỏi: \"Hội nghị COP26 có sự tham gia của những quốc gia nào?\"\n",
    "Đầu ra: {{ \"entities\": [\"COP26\", \"quốc gia\"] }}\n",
    "\n",
    "Câu hỏi: \"Người sáng lập Microsoft là ai?\"\n",
    "Đầu ra: {{ \"entities\": [\"người sáng lập\", \"Microsoft\"] }}\n",
    "\n",
    "#####\n",
    "Hãy trích xuất từ câu hỏi: {question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_entities(question):\n",
    "    formatted_prompt = prompt.format(question=question)\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    # Get the response content or fallback to string representation\n",
    "    response_text = response.content if hasattr(response, \"content\") else str(response)\n",
    "    \n",
    "    # Clean response text by removing unnecessary parts\n",
    "    cleaned_response = re.sub(r\"```json|```\", \"\", response_text).strip()\n",
    "    \n",
    "    try:\n",
    "        # Try parsing the cleaned response as JSON\n",
    "        parsed_response = json.loads(cleaned_response)\n",
    "        \n",
    "        # Ensure that \"entities\" field exists in parsed response\n",
    "        if \"entities\" not in parsed_response:\n",
    "            return {\"error\": \"No 'entities' field found in response\", \"raw\": cleaned_response}\n",
    "        \n",
    "        # Map 'entities' to 'names' field for the Entities model\n",
    "        parsed_response[\"names\"] = parsed_response.pop(\"entities\")\n",
    "        \n",
    "        # Return the entities as a structured object\n",
    "        return Entities(**parsed_response)\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\"error\": f\"JSON Decode Error: {str(e)}\", \"raw\": cleaned_response}\n",
    "    except KeyError as e:\n",
    "        return {\"error\": f\"Missing key: {str(e)}\", \"raw\": cleaned_response}\n",
    "    except TypeError as e:\n",
    "        return {\"error\": f\"Type Error: {str(e)}\", \"raw\": cleaned_response}\n",
    "\n",
    "# Test the function with a question\n",
    "question = \"Lĩnh vực nghiên cứu nổi bật của thầy Lê Hồng Phương? Có phải xử lý ngôn ngữ tự nhiên không?\"\n",
    "entities = extract_entities(question)\n",
    "\n",
    "# Print out the extracted entities or error\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dY8huoM8tHNX"
   },
   "outputs": [],
   "source": [
    "def graph_retriever(question: str) -> str:\n",
    "    result = []\n",
    "    \n",
    "    entities = extract_entities(question)\n",
    "    print(entities)\n",
    "    if isinstance(entities, dict):  # If extraction failed\n",
    "        return \"Có vẻ không có đủ thông tin cho câu hỏi này.\"\n",
    "    \n",
    "    for entity in entities.names:\n",
    "        formatted_query = entity.replace(\" \", \"_\")  # Convert spaces to underscores\n",
    "        fuzzy_query = f\"{formatted_query}*\"  # Add wildcard for partial matching\n",
    "        query_length = len(formatted_query)  # Compute query length in Python\n",
    "\n",
    "        response = graph.query(\n",
    "            \"\"\"\n",
    "CALL db.index.fulltext.queryNodes('fulltext_entity_index', $query, {limit:10}) \n",
    "YIELD node, score\n",
    "WITH node, score, $query_length AS query_length, size(node.id) AS id_length\n",
    "WHERE score >= 0.8 OR (toFloat(query_length) / id_length) >= 0.8\n",
    "CALL {\n",
    "  WITH node\n",
    "  MATCH (node)-[r]->(neighbor)\n",
    "  RETURN CASE \n",
    "    WHEN 'Document' IN labels(node) \n",
    "    THEN coalesce([label IN labels(node) WHERE label <> '__Entity__'][0], 'Unknown') + \": \" + node.source + ' - ' + type(r) + ' -> ' + neighbor.id \n",
    "    ELSE node.id + ' - ' + type(r) + ' -> ' + neighbor.id\n",
    "  END AS output\n",
    "  UNION ALL\n",
    "  WITH node\n",
    "  MATCH (node)<-[r]-(neighbor)\n",
    "  RETURN CASE \n",
    "    WHEN 'Document' IN labels(neighbor) \n",
    "    THEN coalesce([label IN labels(neighbor) WHERE label <> '__Entity__'][0], 'Unknown') + \": \" + neighbor.source + ' - ' + type(r) + ' -> ' + node.id\n",
    "    ELSE neighbor.id + ' - ' + type(r) + ' -> ' + node.id\n",
    "  END AS output\n",
    "}\n",
    "RETURN output LIMIT 15\n",
    "            \"\"\",\n",
    "            {\"query\": fuzzy_query, \"query_length\": query_length},\n",
    "        )\n",
    "        \n",
    "        if response:\n",
    "            filtered = [el['output'].replace(\"_\", \" \") for el in response if not re.search(r\"^[0-9a-f]{32}\", el['output'])]\n",
    "            result.extend(filtered)\n",
    "    \n",
    "    return \"\\n\".join(result) if result else \"Có vẻ chưa có đủ thông tin cho câu hỏi này.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6fOJRPntHNX",
    "outputId": "a99ffca0-2d4d-4374-8519-c6e37c395f1f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 23:21:36,685 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names=['IELTS', 'điểm']\n",
      "Có vẻ chưa có đủ thông tin cho câu hỏi này.\n"
     ]
    }
   ],
   "source": [
    "print(graph_retriever(\"Cần bao nhiêu điểm IELTS?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iCTMp3prtHNX"
   },
   "outputs": [],
   "source": [
    "def full_retriever(question: str):\n",
    "    graph_data = graph_retriever(question)\n",
    "    vector_data = [el.page_content for el in vector_retriever.invoke(\"Hãy tìm các văn bản có liên quan đến câu hỏi sau nhất. \" + question)]\n",
    "    \n",
    "    final_data = f\"\"\"\n",
    "{graph_data}\n",
    "{\"\".join(vector_data)}\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace exact matches before returning\n",
    "    final_data = final_data.replace(\"Document:\", \"Tài liệu:\").replace(\"page_content:\", \"Thông tin:\")\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 23:21:39,342 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names=['trường', 'nghiên cứu', 'lý thuyết thuần túy']\n",
      "\n",
      "Tối ưu hóa và tính toán khoa học - Chứa -> phương pháp giải bài toán bất đẳng thức biến phân\n",
      "Tài liệu: tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md - Nhắc đến -> phương pháp giải bài toán bất đẳng thức biến phân\n",
      "Tối ưu hóa và tính toán khoa học - Chứa -> thiết kế thuật toán\n",
      "Tài liệu: tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md - Nhắc đến -> thiết kế thuật toán\n",
      "Tối ưu hóa và tính toán khoa học - Chứa -> tính toán song song\n",
      "Tài liệu: tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md - Nhắc đến -> tính toán song song\n",
      "Tối ưu hóa và tính toán khoa học - Chứa -> tối ưu tổ hợp\n",
      "Tài liệu: tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md - Nhắc đến -> tối ưu tổ hợp\n",
      "Giảng viên - Hỗ trợ -> Nghiên cứu\n",
      "Tài liệu: tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md - Nhắc đến -> Nghiên cứu\n",
      "Các nghiên cứu - Có ứng dụng trong -> ngành công nghiệp\n",
      "Tài liệu: tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md - Nhắc đến -> Các nghiên cứu\n",
      "\n",
      "Thông tin: trong lý thuyết mật mã. Ngoài ra, Lý thuyết biểu diễn và Tôpô đại số cũng là một mảng nghiên cứu mạnh, bao gồm lý thuyết bất biến modular, lý thuyết đồng luân, và các ứng dụng trong nhiều bài toán khác nhau.\n",
      "Về Cơ học, các nhà nghiên cứu của khoa tập trung vào Cơ học Vật rắn và\n",
      "Thông tin: môn cao trong nhiều lĩnh vực Toán học, Cơ học và Khoa học Máy tính. Các lĩnh vực nghiên cứu trải dài từ lý thuyết thuần túy đến ứng dụng thực tiễn, mở ra nhiều cơ hội cho những ai có niềm đam mê khám phá tri thức và đóng góp vào sự phát triển khoa học.\n",
      "Trong lĩnh vực\n",
      "Thông tin: các mô hình thống kê trong các lĩnh vực khác nhau. Khoa cũng có những nhà nghiên cứu hàng đầu trong lĩnh vực Hình học Đại số và Giải tích Hình học, nghiên cứu về lý thuyết kỳ dị, tích phân motivic, hình học phi Archimedean, và các ứng dụng trong lý thuyết mật mã. Ngoài ra, Lý thuyết biểu\n",
      "Thông tin: phương pháp giải tích số. Bên cạnh đó, các nhà nghiên cứu trong lĩnh vực Xác suất và Thống kê Toán học tập trung vào lý thuyết xác suất, thống kê toán học, các định lý giới hạn, thiết kế thí nghiệm, phân tích dữ liệu, và ứng dụng của các mô hình thống kê trong các lĩnh vực khác\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(full_retriever(\"Trường có nghiên cứ về lý thuyết thuần túy không?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Dzb2jcittHNY"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# LM Studio API settings\n",
    "LM_STUDIO_URL = \"http://127.0.0.1:8000/v1/chat/completions\"  # Update if needed\n",
    "\n",
    "def query_lm_studio(question):\n",
    "    \"\"\"\n",
    "    Fetches context using full_retriever() and queries LM Studio for a response.\n",
    "    \"\"\"\n",
    "    context = full_retriever(question)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Bạn là trợ lý cho Trường Đại học Khoa học Tự nhiên - Đại học Quốc gia Hà Nội.\n",
    "Trang Web của trường: https://hus.vnu.edu.vn/\n",
    "Hãy trả lời câu hỏi của người dùng dựa trên các thông tin được cung cấp.\n",
    "Hãy nghĩ từng bước, xem xét tất cả các thông tin được cung cấp để đưa ra câu trả lời.\n",
    "Nghiêm cấm đưa ra thông tin sai lệch.\n",
    "\n",
    "#####\n",
    "THÔNG TIN CUNG CẤP:\n",
    "{context}\n",
    "\n",
    "#####\n",
    "CÂU HỎI NGƯỜI DÙNG:\n",
    "{question}\n",
    "Hãy trả lời bằng tiếng Việt.\n",
    "\"\"\"\n",
    "\n",
    "    # Call LM Studio's API\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0.5\n",
    "    }\n",
    "    response = requests.post(LM_STUDIO_URL, json=payload)\n",
    "\n",
    "    print(f\"\\nContext:\\n{context}\")\n",
    "    print(\"ANSWER\" + \"=\" * 90 + \"\\n\")\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        return f\"Error: {response.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 23:21:59,393 - INFO - HTTP Request: POST http://127.0.0.1:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names=['trường', 'học']\n",
      "\n",
      "Context:\n",
      "\n",
      "Giải tích Hình học - Là một lĩnh vực -> Học thuật\n",
      "Hình học Đại số - Là một lĩnh vực -> Học thuật\n",
      "Tài liệu: tailieu\\2025-01-25-thong-tin-nghien-cuu-mim.md - Nhắc đến -> Học thuật\n",
      "\n",
      "Thông tin: liệu và y tế, và ứng dụng biểu diễn thưa trong xử lý ảnh. Khoa cũng có các nhà nghiên cứu trong Tối ưu hóa và Tính toán Khoa học, nghiên cứu về thiết kế thuật toán, tối ưu tổ hợp, tính toán song song, và các phương pháp giải bài toán bất đẳng thức biến phân. Tất cả các\n",
      "Thông tin: phương pháp giải tích số. Bên cạnh đó, các nhà nghiên cứu trong lĩnh vực Xác suất và Thống kê Toán học tập trung vào lý thuyết xác suất, thống kê toán học, các định lý giới hạn, thiết kế thí nghiệm, phân tích dữ liệu, và ứng dụng của các mô hình thống kê trong các lĩnh vực khác\n",
      "Thông tin: bài toán bất đẳng thức biến phân. Tất cả các lĩnh vực nghiên cứu này được hỗ trợ bởi một đội ngũ giảng viên giàu kinh nghiệm và các trang thiết bị hiện đại, tạo điều kiện cho sinh viên và các nhà nghiên cứu phát triển hết tiềm năng.\n",
      "Thông tin: môn cao trong nhiều lĩnh vực Toán học, Cơ học và Khoa học Máy tính. Các lĩnh vực nghiên cứu trải dài từ lý thuyết thuần túy đến ứng dụng thực tiễn, mở ra nhiều cơ hội cho những ai có niềm đam mê khám phá tri thức và đóng góp vào sự phát triển khoa học.\n",
      "Trong lĩnh vực\n",
      "    \n",
      "ANSWER==========================================================================================\n",
      "\n",
      "Trường Đại học Khoa học Tự nhiên - Đại học Quốc gia Hà Nội là một cơ sở giáo dục bậc đại học danh giá, với nhiều lĩnh vực nghiên cứu Toán học đa dạng và ứng dụng thực tiễn. Về vấn đề học thuật liên quan đến thuật toán, trường cung cấp nhiều khóa học và chương trình nghiên cứu trong lĩnh vực Khoa học Máy tính và Toán học Ứng dụng.\n",
      "\n",
      "Các khóa học liên quan đến thuật toán thường được giảng dạy trong ngành Khoa học Máy tính hoặc Toán học Máy tính, nơi sinh viên được đào tạo về thiết kế và phân tích thuật toán, cấu trúc dữ liệu và giải quyết vấn đề máy tính. Một số khóa học điển hình có thể bao gồm:\n",
      "\n",
      "1. Giải thuật và Cấu trúc Dữ liệu: Khóa học này tập trung vào các nguyên tắc cơ bản của thiết kế thuật toán, phân tích độ phức tạp thời gian và không gian của các thuật toán, cũng như lựa chọn cấu trúc dữ liệu phù hợp cho các vấn đề cụ thể.\n",
      "\n",
      "2. Tối ưu Hóa và Lý thuyết Đồ thị: Khóa học này khám phá các kỹ thuật tối ưu hóa toán học, bao gồm lập trình tuyến tính, động lực học, và lý thuyết đồ thị. Sinh viên tìm hiểu cách áp dụng các phương pháp này để giải quyết các vấn đề thực tế trong tối ưu hóa tài nguyên, mạng lưới vận chuyển và nhiều lĩnh vực khác.\n",
      "\n",
      "3. Xử Lý Ảnh Số: Trong khóa học này, sinh viên tìm hiểu về các thuật toán xử lý ảnh số, bao gồm xử lý tín hiệu hình ảnh, nhận dạng mẫu, và phân tích dữ liệu hình ảnh. Đây là một lĩnh vực ứng dụng rộng rãi trong thị giác máy tính, y tế, và đồ họa máy tính.\n",
      "\n",
      "4. Học Máy và Trí tuệ Nhân tạo: Khóa học này giới thiệu cho sinh viên về các nguyên lý cơ bản của học máy, bao gồm phân loại, hồi quy, và học không giám sát. Sinh viên cũng có thể tìm hiểu về các thuật toán trí tuệ nhân tạo, như mạng nơ-ron nhân tạo và xử lý ngôn ngữ tự nhiên.\n",
      "\n",
      "5. Thống Kê Toán Học: Khóa học này tập trung vào các phương pháp thống kê cho dữ liệu lớn và phức tạp. Sinh viên tìm hiểu về lý thuyết xác suất, phân phối xác suất, kiểm định giả thuyết và phân tích hồi quy, với ứng dụng trong khoa học dữ liệu, nghiên cứu thị trường và nhiều lĩnh vực khác.\n",
      "\n",
      "Ngoài ra, trường Đại học Khoa học Tự nhiên còn có các phòng thí nghiệm máy tính và trung tâm nghiên cứu tiên tiến, nơi sinh viên có thể áp dụng kiến thức thuật toán vào các dự án thực tế và hợp tác với giảng viên trong các lĩnh vực như trí tuệ nhân tạo, học máy, xử lý ngôn ngữ tự nhiên và nhiều hơn nữa.\n",
      "\n",
      "Để biết thêm chi tiết cụ thể về các khóa học và chương trình liên quan đến thuật toán tại trường Đại học Khoa học Tự nhiên - Đại học Quốc gia Hà Nội, bạn có thể tham khảo trang web chính thức của trường hoặc liên hệ trực tiếp với khoa/bộ môn liên quan.\n"
     ]
    }
   ],
   "source": [
    "print(query_lm_studio(\"Có thông tin gì về việc học các thuật toán trong trường?\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AutoKG (Python 3.10)",
   "language": "python",
   "name": "autokg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "8e37edd9789a4d57a7be401628e7ff7f": {
     "model_module": "yfiles-jupyter-graphs",
     "model_module_version": "^1.6.1",
     "model_name": "GraphModel",
     "state": {
      "_context_pane_mapping": [
       {
        "id": "Neighborhood",
        "title": "Neighborhood"
       },
       {
        "id": "Data",
        "title": "Data"
       },
       {
        "id": "Search",
        "title": "Search"
       },
       {
        "id": "About",
        "title": "About"
       }
      ],
      "_data_importer": "neo4j",
      "_directed": true,
      "_dom_classes": [],
      "_edges": [
       {
        "color": "#9C27B0",
        "directed": true,
        "end": 2,
        "id": 14,
        "label": "RULED",
        "properties": {
         "label": "RULED"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#9C27B0",
        "directed": true,
        "end": 3,
        "id": 15,
        "label": "RULED",
        "properties": {
         "label": "RULED"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#2196F3",
        "directed": true,
        "end": 4,
        "id": 16,
        "label": "BELONGED_TO",
        "properties": {
         "label": "BELONGED_TO"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 5,
        "id": 17,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 6,
        "id": 18,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 6,
        "id": 19,
        "label": "SPOUSE",
        "properties": {
         "label": "SPOUSE"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 7,
        "id": 20,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#607D8B",
        "directed": true,
        "end": 8,
        "id": 21,
        "label": "BEQUEATHED_CROWN_TO",
        "properties": {
         "label": "BEQUEATHED_CROWN_TO"
        },
        "start": 7,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 9,
        "id": 22,
        "label": "IGNORED_CLAIMS_OF",
        "properties": {
         "label": "IGNORED_CLAIMS_OF"
        },
        "start": 7,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 1,
        "id": 23,
        "label": "IGNORED_CLAIMS_OF",
        "properties": {
         "label": "IGNORED_CLAIMS_OF"
        },
        "start": 7,
        "thickness_factor": 1
       },
       {
        "color": "#CDDC39",
        "directed": true,
        "end": 1,
        "id": 24,
        "label": "IMPRISONED",
        "properties": {
         "label": "IMPRISONED"
        },
        "start": 9,
        "thickness_factor": 1
       },
       {
        "color": "#9E9E9E",
        "directed": true,
        "end": 10,
        "id": 25,
        "label": "DEPENDED_ON",
        "properties": {
         "label": "DEPENDED_ON"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#9C27B0",
        "directed": true,
        "end": 11,
        "id": 26,
        "label": "CREATED_TITLE",
        "properties": {
         "label": "CREATED_TITLE"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#2196F3",
        "directed": true,
        "end": 12,
        "id": 27,
        "label": "SUCCEEDED_BY",
        "properties": {
         "label": "SUCCEEDED_BY"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 12,
        "id": 28,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 13,
        "thickness_factor": 1
       },
       {
        "color": "#9E9E9E",
        "directed": true,
        "end": 14,
        "id": 29,
        "label": "DEPENDED_ON",
        "properties": {
         "label": "DEPENDED_ON"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 17,
        "id": 40,
        "label": "WAR",
        "properties": {
         "label": "WAR"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 18,
        "id": 41,
        "label": "WAR",
        "properties": {
         "label": "WAR"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 19,
        "id": 42,
        "label": "WAR",
        "properties": {
         "label": "WAR"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 3,
        "id": 43,
        "label": "WAR",
        "properties": {
         "label": "WAR"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 20,
        "id": 44,
        "label": "LEAD",
        "properties": {
         "label": "LEAD"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 21,
        "id": 45,
        "label": "LEAD",
        "properties": {
         "label": "LEAD"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 22,
        "id": 46,
        "label": "LEAD",
        "properties": {
         "label": "LEAD"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 23,
        "id": 47,
        "label": "LEAD",
        "properties": {
         "label": "LEAD"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#607D8B",
        "directed": true,
        "end": 24,
        "id": 48,
        "label": "DEFEAT",
        "properties": {
         "label": "DEFEAT"
        },
        "start": 16,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 12,
        "id": 64,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 1,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 7,
        "id": 65,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 29,
        "id": 66,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 1,
        "id": 67,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 30,
        "id": 68,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 31,
        "id": 69,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 32,
        "id": 70,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#673AB7",
        "directed": true,
        "end": 33,
        "id": 71,
        "label": "SUCCESSOR",
        "properties": {
         "label": "SUCCESSOR"
        },
        "start": 5,
        "thickness_factor": 1
       },
       {
        "color": "#CDDC39",
        "directed": true,
        "end": 34,
        "id": 72,
        "label": "PREFERRED_SUCCESSOR",
        "properties": {
         "label": "PREFERRED_SUCCESSOR"
        },
        "start": 29,
        "thickness_factor": 1
       },
       {
        "color": "#9E9E9E",
        "directed": true,
        "end": 35,
        "id": 73,
        "label": "FAMILY_RELATION",
        "properties": {
         "label": "FAMILY_RELATION"
        },
        "start": 34,
        "thickness_factor": 1
       },
       {
        "color": "#9C27B0",
        "directed": true,
        "end": 13,
        "id": 74,
        "label": "MARRIAGE",
        "properties": {
         "label": "MARRIAGE"
        },
        "start": 35,
        "thickness_factor": 1
       },
       {
        "color": "#2196F3",
        "directed": true,
        "end": 37,
        "id": 91,
        "label": "GRANDPARENT",
        "properties": {
         "label": "GRANDPARENT"
        },
        "start": 39,
        "thickness_factor": 1
       },
       {
        "color": "#2196F3",
        "directed": true,
        "end": 38,
        "id": 92,
        "label": "GRANDPARENT",
        "properties": {
         "label": "GRANDPARENT"
        },
        "start": 39,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 40,
        "id": 93,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 34,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 38,
        "id": 94,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 40,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 37,
        "id": 95,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 13,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 16,
        "id": 96,
        "label": "CONTENDER",
        "properties": {
         "label": "CONTENDER"
        },
        "start": 13,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 8,
        "id": 97,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 41,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 46,
        "id": 98,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 41,
        "thickness_factor": 1
       },
       {
        "color": "#4CAF50",
        "directed": true,
        "end": 47,
        "id": 99,
        "label": "PARENT",
        "properties": {
         "label": "PARENT"
        },
        "start": 41,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 41,
        "id": 100,
        "label": "SPOUSE",
        "properties": {
         "label": "SPOUSE"
        },
        "start": 44,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 42,
        "id": 101,
        "label": "SPOUSE",
        "properties": {
         "label": "SPOUSE"
        },
        "start": 44,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 41,
        "id": 102,
        "label": "SPOUSE",
        "properties": {
         "label": "SPOUSE"
        },
        "start": 45,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 48,
        "id": 103,
        "label": "SPOUSE",
        "properties": {
         "label": "SPOUSE"
        },
        "start": 46,
        "thickness_factor": 1
       },
       {
        "color": "#F44336",
        "directed": true,
        "end": 51,
        "id": 117,
        "label": "HELD_TITLE",
        "properties": {
         "label": "HELD_TITLE"
        },
        "start": 50,
        "thickness_factor": 1
       }
      ],
      "_graph_layout": {},
      "_highlight": [],
      "_license": {},
      "_model_module": "yfiles-jupyter-graphs",
      "_model_module_version": "^1.6.1",
      "_model_name": "GraphModel",
      "_neighborhood": {},
      "_nodes": [
       {
        "color": "#2196F3",
        "id": 1,
        "label": "Elizabeth I",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Elizabeth I",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#4CAF50",
        "id": 2,
        "label": "England",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "England",
         "label": "Country:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#4CAF50"
       },
       {
        "color": "#4CAF50",
        "id": 3,
        "label": "Ireland",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Ireland",
         "label": "Country:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#4CAF50"
       },
       {
        "color": "#F44336",
        "id": 4,
        "label": "House Of Tudor",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "House Of Tudor",
         "label": "__Entity__:Royal family"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#F44336"
       },
       {
        "color": "#2196F3",
        "id": 5,
        "label": "Henry Viii",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Henry Viii",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 6,
        "label": "Anne Boleyn",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Anne Boleyn",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 7,
        "label": "Edward Vi",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Edward Vi",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 8,
        "label": "Lady Jane Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Lady Jane Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 9,
        "label": "Mary",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Mary",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 10,
        "label": "William Cecil",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "William Cecil",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#607D8B",
        "id": 11,
        "label": "Baron Burghley",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Baron Burghley",
         "label": "Title:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#607D8B"
       },
       {
        "color": "#2196F3",
        "id": 12,
        "label": "James Vi Of Scotland",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "James Vi Of Scotland",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 13,
        "label": "Mary, Queen Of Scots",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Mary, Queen Of Scots",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 14,
        "label": "Francis Walsingham",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Francis Walsingham",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 16,
        "label": "Elizabeth",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Elizabeth",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#4CAF50",
        "id": 17,
        "label": "Spain",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Spain",
         "label": "Country:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#4CAF50"
       },
       {
        "color": "#4CAF50",
        "id": 18,
        "label": "Netherlands",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Netherlands",
         "label": "Country:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#4CAF50"
       },
       {
        "color": "#4CAF50",
        "id": 19,
        "label": "France",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "France",
         "label": "Country:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#4CAF50"
       },
       {
        "color": "#673AB7",
        "id": 20,
        "label": "William Shakespeare",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "William Shakespeare",
         "label": "Playwright:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#673AB7"
       },
       {
        "color": "#673AB7",
        "id": 21,
        "label": "Christopher Marlowe",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Christopher Marlowe",
         "label": "Playwright:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#673AB7"
       },
       {
        "color": "#CDDC39",
        "id": 22,
        "label": "Francis Drake",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Francis Drake",
         "label": "Explorer:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#CDDC39"
       },
       {
        "color": "#CDDC39",
        "id": 23,
        "label": "Walter Raleigh",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Walter Raleigh",
         "label": "Explorer:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#CDDC39"
       },
       {
        "color": "#9E9E9E",
        "id": 24,
        "label": "Spanish Armada",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Spanish Armada",
         "label": "Event:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#9E9E9E"
       },
       {
        "color": "#2196F3",
        "id": 29,
        "label": "Mary I",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Mary I",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 30,
        "label": "Jane Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Jane Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 31,
        "label": "Katherine Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Katherine Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 32,
        "label": "Mary Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Mary Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 33,
        "label": "Margaret Clifford",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Margaret Clifford",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 34,
        "label": "Margaret Douglas",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Margaret Douglas",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 35,
        "label": "Henry Stuart, Lord Darnley",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Henry Stuart, Lord Darnley",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 39,
        "label": "Margaret Tudor",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Margaret Tudor",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 37,
        "label": "James Vi",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "James Vi",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 38,
        "label": "Arbella Stuart",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Arbella Stuart",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 40,
        "label": "Charles Stuart",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Charles Stuart",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 41,
        "label": "Frances Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Frances Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 46,
        "label": "Lady Catherine Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Lady Catherine Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 47,
        "label": "Lady Mary Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Lady Mary Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 44,
        "label": "Charles Brandon",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Charles Brandon",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 42,
        "label": "Eleanor Clifford",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Eleanor Clifford",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 45,
        "label": "Henry Grey",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Henry Grey",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 48,
        "label": "Henry Herbert",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Henry Herbert",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#2196F3",
        "id": 50,
        "label": "Elizabeth Petrovna",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Elizabeth Petrovna",
         "label": "Person:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#2196F3"
       },
       {
        "color": "#607D8B",
        "id": 51,
        "label": "Empress Of Russia",
        "position": [
         0,
         0
        ],
        "properties": {
         "id": "Empress Of Russia",
         "label": "Title:__Entity__"
        },
        "scale_factor": 1,
        "size": [
         55,
         55
        ],
        "styles": {},
        "type": "#607D8B"
       }
      ],
      "_overview": {
       "enabled": null,
       "overview_set": false
      },
      "_selected_graph": [
       [],
       []
      ],
      "_sidebar": {
       "enabled": true,
       "start_with": ""
      },
      "_view_count": null,
      "_view_module": "yfiles-jupyter-graphs",
      "_view_module_version": "^1.6.1",
      "_view_name": "GraphView",
      "layout": "IPY_MODEL_9bac7003afd84cecb4e67a81a396ec8d"
     }
    },
    "9bac7003afd84cecb4e67a81a396ec8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": "800px",
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
